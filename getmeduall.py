import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
from io import BytesIO
from docx import Document
from docx.shared import Inches
import pandas as pd

# === å˜å…ƒæƒ…å ±ã®èª­ã¿è¾¼ã¿ ===
@st.cache_data
def load_unit_mapping(uploaded_file):
    df = pd.read_csv(uploaded_file)
    return dict(zip(df['å•é¡Œç•ªå·'], df['å˜å…ƒå']))

# === ãƒšãƒ¼ã‚¸å–å¾—é–¢æ•° ===
def get_page_text(url, qid, unit_map, get_images=True):
    try:
        resp = requests.get(url)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, 'html.parser')
        category = unit_map.get(qid, soup.find('span', class_='button-small-line').text.strip() if soup.find('span', class_='button-small-line') else 'åˆ†é‡åãªã—')
        problem = soup.find('div', class_='quiz-body mb-64')
        choices = [f"{c.find('span', class_='choice-header').text.strip()} {c.find_all('span')[1].text.strip()}"
                   for c in soup.find_all('div', class_='box-select')]
        h4s = soup.find_all('h4')
        ans = h4s[0].text.strip() if h4s else 'è§£ç­”ãªã—'
        expl = soup.find('div', class_='explanation').text.strip() if soup.find('div', class_='explanation') else 'è§£èª¬ãªã—'
        imgs = []
        if get_images:
            for d in soup.find_all('div', class_='box-quiz-image mb-32'):
                img = d.find('img')
                if img and img.get('src'):
                    imgs.append(img['src'].replace('thumb_', ''))
        return {
            "category": category,
            "problem": problem.text.strip() if problem else 'å•é¡Œæ–‡ãªã—',
            "choices": choices,
            "answer": ans,
            "question_id": qid,
            "explanation": expl,
            "images": imgs
        }
    except:
        return None

# === Wordç”Ÿæˆé–¢æ•° ===
def create_word_doc(pages, year, label, include_images=True):
    doc = Document()
    doc.add_heading(f'{year}å¹´ åŒ»å¸«å›½å®¶è©¦é¨“å•é¡Œï¼ˆ{label}ï¼‰', 0)
    doc.add_paragraph(f"å–å¾—å•é¡Œæ•°: {len(pages)}å•")
    for i, p in enumerate(pages, 1):
        doc.add_heading(f"å•é¡Œ{ i } {p['question_id']}", level=2)
        doc.add_paragraph(f"å˜å…ƒ: {p['category']}")
        doc.add_paragraph(p['problem'])
        if include_images and p['images']:
            for url in p['images']:
                try:
                    r = requests.get(url)
                    if r.status_code == 200:
                        img_stream = BytesIO(r.content)
                        doc.add_picture(img_stream, width=Inches(2.5))
                except:
                    pass
        doc.add_paragraph("é¸æŠè‚¢ï¼š")
        for c in p['choices']:
            doc.add_paragraph(c)
        doc.add_paragraph(p['answer'])
        doc.add_paragraph("è§£èª¬: " + p['explanation'])
        doc.add_page_break()
    fn = f"{year}_{label}_medu4.docx"
    doc.save(fn)
    return fn

# === ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®å–å¾—é–¢æ•° ===
def scrape_sections(year, sections, unit_map, include_images=True):
    collected = []
    for sec_idx, sec in enumerate(sections):
        st.markdown(f"### â–¶ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {sec} ã‚’å–å¾—ä¸­...")
        bar = st.progress(0)
        fail_count = 0
        for i, num in enumerate(range(1, 81)):
            qid = f"{year}{sec}{num}"
            url = f"https://medu4.com/{qid}"
            data = get_page_text(url, qid, unit_map, get_images=include_images)
            if data:
                collected.append(data)
                fail_count = 0
            else:
                fail_count += 1
                if fail_count >= 3:
                    st.warning(f"âš  {sec}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§3å•é€£ç¶šå¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    break
            bar.progress((i + 1) / 80)
            time.sleep(0.15)
        bar.empty()
    return collected

# === Streamlit UI ===
st.title("ğŸ©º å›½è©¦å•é¡Œå–å¾—ãƒ„ãƒ¼ãƒ« + å˜å…ƒåˆ†é¡ (from CSV)")

uploaded_file = st.file_uploader("å•é¡Œç•ªå·ã¨å˜å…ƒåã®CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
year = st.text_input("å¹´åº¦ã‚’å…¥åŠ›ï¼ˆä¾‹: 100ï¼‰")
include_images = st.checkbox("ç”»åƒã‚‚å–å¾—ã™ã‚‹", value=True)

if uploaded_file and year:
    unit_map = load_unit_mapping(uploaded_file)
    col1, col2 = st.columns(2)

    with col1:
        if st.button("Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"):
            ac_sections = ['A', 'B', 'C']
            with st.spinner("Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ä¸­..."):
                ac_data = scrape_sections(year, ac_sections, unit_map, include_images)
                if ac_data:
                    fn_ac = create_word_doc(ac_data, year, "A-C", include_images)
                    st.success("âœ… Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                    with open(fn_ac, "rb") as f:
                        st.download_button("Aã€œCã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_ac)

    with col2:
        if st.button("Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"):
            di_sections = ['D', 'E', 'F', 'G', 'H', 'I']
            with st.spinner("Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ä¸­..."):
                di_data = scrape_sections(year, di_sections, unit_map, include_images)
                if di_data:
                    fn_di = create_word_doc(di_data, year, "D-I", include_images)
                    st.success("âœ… Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                    with open(fn_di, "rb") as f:
                        st.download_button("Dã€œIã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_di)
