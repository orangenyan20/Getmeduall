import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
from io import BytesIO
from docx import Document
from docx.shared import Inches

# â­ GitHubã®CSV URLï¼ˆorangeã¡ã‚ƒã‚“ã®GitHub URLã«å·®ã—æ›¿ãˆã¦ã­ï¼‰
CSV_URL = "https://raw.githubusercontent.com/orangenyan20/Getmeduall/main/data.csv"

@st.cache_data
def load_topic_mapping():
    try:
        df = pd.read_csv(CSV_URL)
        mapping = dict(zip(df.iloc[:, 0].astype(str), df.iloc[:, 1]))
        return mapping
    except Exception as e:
        st.error(f"å˜å…ƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

def get_page_text(url, get_images=True):
    try:
        resp = requests.get(url)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, 'html.parser')
        problem = soup.find('div', class_='quiz-body mb-64')
        choices = [f"{c.find('span', class_='choice-header').text.strip()} {c.find_all('span')[1].text.strip()}"
                   for c in soup.find_all('div', class_='box-select')]
        h4s = soup.find_all('h4')
        ans = h4s[0].text.strip() if h4s else 'è§£ç­”ãªã—'
        qid = re.search(r'([0-9]{3}[A-Za-z][0-9]+)', h4s[1].text).group(1) if len(h4s) >=2 else 'å•é¡Œç•ªå·ãªã—'
        expl = soup.find('div', class_='explanation').text.strip() if soup.find('div', class_='explanation') else 'è§£èª¬ãªã—'
        imgs = []

        if get_images:
            for d in soup.find_all('div', class_='box-quiz-image'):
                for a in d.find_all('a', href=True):
                    href = a['href']
                    if '.jpg' in href:
                        imgs.append(href.replace('thumb_', ''))

        return {
            "question_id": qid,
            "problem": problem.text.strip() if problem else 'å•é¡Œæ–‡ãªã—',
            "choices": choices,
            "answer": ans,
            "explanation": expl,
            "images": imgs
        }
    except:
        return None

def create_word_doc(pages, year, label, topic_map, include_images=True):
    doc = Document()
    doc.add_heading(f'{year}å¹´ åŒ»å¸«å›½å®¶è©¦é¨“å•é¡Œï¼ˆ{label}ï¼‰', 0)
    doc.add_paragraph(f"å–å¾—å•é¡Œæ•°: {len(pages)}å•")
    for i, p in enumerate(pages, 1):
        doc.add_heading(f"å•é¡Œ{ i } {p['question_id']}", level=2)
        unit = topic_map.get(p['question_id'], "åˆ†é‡åãªã—")
        doc.add_paragraph(f"åˆ†é‡: {unit}")
        doc.add_paragraph(p['problem'])
        if include_images and p['images']:
            for url in p['images']:
                try:
                    r = requests.get(url)
                    if r.status_code == 200:
                        img_stream = BytesIO(r.content)
                        doc.add_picture(img_stream, width=Inches(2.5))
                except:
                    pass
        doc.add_paragraph("é¸æŠè‚¢ï¼š")
        for c in p['choices']:
            doc.add_paragraph(c)
        doc.add_paragraph(p['answer'])
        doc.add_paragraph("è§£èª¬: " + p['explanation'])
        doc.add_page_break()
    fn = f"{year}_{label}_medu4.docx"
    doc.save(fn)
    return fn

def scrape_sections(year, sections, topic_map, include_images=True):
    collected = []
    for sec in sections:
        st.markdown(f"### â–¶ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {sec} ã‚’å–å¾—ä¸­...")
        bar = st.progress(0)
        fail_count = 0
        for i, num in enumerate(range(1, 81)):
            qid = f"{year}{sec}{num}"
            url = f"https://medu4.com/{qid}"
            data = get_page_text(url, get_images=include_images)
            if data:
                data["question_id"] = qid
                collected.append(data)
                fail_count = 0
            else:
                fail_count += 1
                if fail_count >= 3:
                    st.warning(f"âš  {sec}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§3å•é€£ç¶šå¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    break
            bar.progress((i + 1) / 80)
            time.sleep(0.15)
        bar.empty()
    return collected

# ğŸ›ï¸ UI
st.title("ğŸ©º å›½è©¦å•é¡Œå–å¾—ãƒ„ãƒ¼ãƒ«ï¼ˆGitHubé€£æºç‰ˆï¼‰")
year = st.text_input("å¹´åº¦ã‚’å…¥åŠ›ï¼ˆä¾‹: 100ï¼‰")
include_images = st.checkbox("ç”»åƒã‚‚å–å¾—ã™ã‚‹", value=True)

# GitHubã‹ã‚‰å˜å…ƒãƒ‡ãƒ¼ã‚¿å–å¾—
topic_map = load_topic_mapping()

col1, col2 = st.columns(2)

with col1:
    if st.button("Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—é–‹å§‹"):
        if year:
            ac_data = scrape_sections(year, ['A', 'B', 'C'], topic_map, include_images)
            if ac_data:
                fn_ac = create_word_doc(ac_data, year, "A-C", topic_map, include_images)
                st.success("âœ… Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                with open(fn_ac, "rb") as f:
                    st.download_button("Aã€œCã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_ac)
            else:
                st.error("âŒ Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æœ‰åŠ¹ãªå•é¡ŒãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

with col2:
    if st.button("Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—é–‹å§‹"):
        if year:
            di_data = scrape_sections(year, ['D', 'E', 'F', 'G', 'H', 'I'], topic_map, include_images)
            if di_data:
                fn_di = create_word_doc(di_data, year, "D-I", topic_map, include_images)
                st.success("âœ… Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                with open(fn_di, "rb") as f:
                    st.download_button("Dã€œIã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_di)
            else:
                st.error("âŒ Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æœ‰åŠ¹ãªå•é¡ŒãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
