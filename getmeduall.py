import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
from io import BytesIO
from docx import Document
from docx.shared import Inches

# ãƒšãƒ¼ã‚¸å–å¾—é–¢æ•°
def get_page_text(url, get_images=True):
    try:
        resp = requests.get(url)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, 'html.parser')
        category = soup.find('span', class_='button-small-line')
        problem = soup.find('div', class_='quiz-body mb-64')
        choices = [f"{c.find('span', class_='choice-header').text.strip()} {c.find_all('span')[1].text.strip()}"
                   for c in soup.find_all('div', class_='box-select')]
        h4s = soup.find_all('h4')
        ans = h4s[0].text.strip() if h4s else 'è§£ç­”ãªã—'
        qid = re.search(r'([0-9]{3}[A-Za-z][0-9]+)', h4s[1].text).group(1) if len(h4s) >=2 else 'å•é¡Œç•ªå·ãªã—'
        expl = soup.find('div', class_='explanation').text.strip() if soup.find('div', class_='explanation') else 'è§£èª¬ãªã—'
        imgs = []
        if get_images:
            for d in soup.find_all('div', class_='box-quiz-image mb-32'):
                img = d.find('img')
                if img and img.get('src'):
                    imgs.append(img['src'].replace('thumb_', ''))
        return {
            "category": category.text.strip() if category else 'åˆ†é‡åãªã—',
            "problem": problem.text.strip() if problem else 'å•é¡Œæ–‡ãªã—',
            "choices": choices,
            "answer": ans,
            "question_id": qid,
            "explanation": expl,
            "images": imgs
        }
    except:
        return None

# Wordç”Ÿæˆé–¢æ•°
def create_word_doc(pages, year, label, include_images=True):
    doc = Document()
    doc.add_heading(f'{year}å¹´ åŒ»å¸«å›½å®¶è©¦é¨“å•é¡Œï¼ˆ{label}ï¼‰', 0)
    doc.add_paragraph(f"å–å¾—å•é¡Œæ•°: {len(pages)}å•")
    for i, p in enumerate(pages, 1):
        doc.add_heading(f"å•é¡Œ{ i } {p['question_id']}", level=2)
        doc.add_paragraph(f"åˆ†é‡: {p['category']}")
        doc.add_paragraph(p['problem'])
        if include_images and p['images']:
            for url in p['images']:
                try:
                    r = requests.get(url)
                    if r.status_code == 200:
                        img_stream = BytesIO(r.content)
                        doc.add_picture(img_stream, width=Inches(2.5))
                except:
                    pass
        doc.add_paragraph("é¸æŠè‚¢ï¼š")
        for c in p['choices']:
            doc.add_paragraph(c)
        doc.add_paragraph(p['answer'])
        doc.add_paragraph("è§£èª¬: " + p['explanation'])
        doc.add_page_break()
    fn = f"{year}_{label}_medu4.docx"
    doc.save(fn)
    return fn

# ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«å‡¦ç†
def scrape_sections(year, sections, include_images=True):
    collected = []
    for sec_idx, sec in enumerate(sections):
        st.markdown(f"### â–¶ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {sec} ã‚’å–å¾—ä¸­...")
        bar = st.progress(0)
        fail_count = 0
        for i, num in enumerate(range(1, 81)):
            qid = f"{year}{sec}{num}"
            url = f"https://medu4.com/{qid}"
            data = get_page_text(url, get_images=include_images)
            if data:
                collected.append(data)
                fail_count = 0
            else:
                fail_count += 1
                if fail_count >= 3:
                    st.warning(f"âš  {sec}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§3å•é€£ç¶šå¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    break
            bar.progress((i + 1) / 80)
            time.sleep(0.15)
        bar.empty()
    return collected

# UI
st.title("ğŸ©º å›½è©¦å•é¡Œå–å¾—ãƒ„ãƒ¼ãƒ«ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰² & é€”ä¸­åœæ­¢å¯¾å¿œï¼‰")

year = st.text_input("å¹´åº¦ã‚’å…¥åŠ›ï¼ˆä¾‹: 100ï¼‰")
include_images = st.checkbox("ç”»åƒã‚‚å–å¾—ã™ã‚‹", value=True)

col1, col2 = st.columns(2)

with col1:
    if st.button("Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—é–‹å§‹"):
        if year:
            ac_sections = ['A', 'B', 'C']
            with st.spinner("Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ä¸­..."):
                ac_data = scrape_sections(year, ac_sections, include_images)
                if ac_data:
                    fn_ac = create_word_doc(ac_data, year, "A-C", include_images)
                    st.success("âœ… Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                    with open(fn_ac, "rb") as f:
                        st.download_button("Aã€œCã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_ac)
                else:
                    st.error("âŒ Aã€œCã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æœ‰åŠ¹ãªå•é¡ŒãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

with col2:
    if st.button("Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—é–‹å§‹"):
        if year:
            di_sections = ['D', 'E', 'F', 'G', 'H', 'I']
            with st.spinner("Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ä¸­..."):
                di_data = scrape_sections(year, di_sections, include_images)
                if di_data:
                    fn_di = create_word_doc(di_data, year, "D-I", include_images)
                    st.success("âœ… Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
                    with open(fn_di, "rb") as f:
                        st.download_button("Dã€œIã®Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", f, file_name=fn_di)
                else:
                    st.error("âŒ Dã€œIã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æœ‰åŠ¹ãªå•é¡ŒãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
